NAME: myotel
LAST DEPLOYED: Sat Nov  5 09:26:44 2022
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
clickhouse:
  affinity: {}
  args: []
  auth:
    existingSecret: ""
    existingSecretKey: ""
    password: ""
    username: default
  clusterDomain: cluster.local
  command: []
  common:
    exampleValue: common-chart
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
  commonAnnotations: {}
  commonLabels: {}
  containerPorts:
    http: 8123
    interserver: 9009
    metrics: 8001
    mysql: 9004
    postgresql: 9005
    tcp: 9000
    tcpSecure: 9440
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    enabled: true
    runAsNonRoot: true
    runAsUser: 1001
  customLivenessProbe: {}
  customReadinessProbe: {}
  customStartupProbe: {}
  defaultConfigurationOverrides: |
    <clickhouse>
      <!-- Macros -->
      <macros>
        <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
        <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
        <layer>{{ include "common.names.fullname" . }}</layer>
      </macros>
      <!-- Log Level -->
      <logger>
        <level>{{ .Values.logLevel }}</level>
      </logger>
      {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <default>
          {{- $shards := $.Values.shards | int }}
          {{- range $shard, $e := until $shards }}
          <shard>
              {{- $replicas := $.Values.replicaCount | int }}
              {{- range $i, $_e := until $replicas }}
              <replica>
                  <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
                  <port>{{ $.Values.service.ports.tcp }}</port>
              </replica>
              {{- end }}
          </shard>
          {{- end }}
        </default>
      </remote_servers>
      {{- end }}
      {{- if or .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
      <!-- Zookeeper configuration -->
      <zookeeper>
        {{- if .Values.zookeeper.enabled }}
        {{/* Zookeeper configuration using the helm chart */}}
        {{- $nodes := .Values.zookeeper.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host>{{ printf "%s-%d.%s.%s.svc.%s" (include "clickhouse.zookeeper.fullname" $ ) $node (include "clickhouse.zookeeper.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
          <port>{{ $.Values.zookeeper.service.ports.client }}</port>
        </node>
        {{- end }}
        {{- else if .Values.externalZookeeper.servers }}
        {{/* Zookeeper configuration using an external instance */}}
        {{- range $node :=.Values.externalZookeeper.servers }}
        <node>
          <host>{{ $node }}</host>
          <port>{{ $.Values.externalZookeeper.port }}</port>
        </node>
        {{- end }}
        {{- end }}
      </zookeeper>
      {{- end }}
      {{- if .Values.tls.enabled }}
      <!-- TLS configuration -->
      <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
      <openSSL>
          <server>
              {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
              {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
              <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
              <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
              <verificationMode>none</verificationMode>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
              {{- $caFileName := default "ca.crt" .Values.tls.certFilename }}
              <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
              {{- else }}
              <loadDefaultCAFile>true</loadDefaultCAFile>
              {{- end }}
          </server>
          <client>
              <loadDefaultCAFile>true</loadDefaultCAFile>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              <verificationMode>none</verificationMode>
              <invalidCertificateHandler>
                  <name>AcceptCertificateHandler</name>
              </invalidCertificateHandler>
          </client>
      </openSSL>
      {{- end }}
      {{- if .Values.metrics.enabled }}
       <!-- Prometheus metrics -->
       <prometheus>
          <endpoint>/metrics</endpoint>
          <port from_env="CLICKHOUSE_METRICS_PORT"></port>
          <metrics>true</metrics>
          <events>true</events>
          <asynchronous_metrics>true</asynchronous_metrics>
      </prometheus>
      {{- end }}
    </clickhouse>
  diagnosticMode:
    args:
    - infinity
    command:
    - sleep
    enabled: false
  enabled: true
  existingOverridesConfigmap: ""
  externalAccess:
    enabled: false
    service:
      annotations: {}
      extraPorts: []
      labels: {}
      loadBalancerAnnotations: []
      loadBalancerIPs: []
      loadBalancerSourceRanges: []
      nodePorts:
        http: []
        interserver: []
        metrics: []
        mysql: []
        postgresql: []
        tcp: []
        tcpSecure: []
      ports:
        http: 80
        interserver: 9009
        metrics: 8001
        mysql: 9004
        postgresql: 9005
        tcp: 9000
        tcpSecure: 9440
      type: LoadBalancer
  externalZookeeper:
    port: 2888
    servers: []
  extraDeploy: []
  extraEnvVars: []
  extraEnvVarsCM: ""
  extraEnvVarsSecret: ""
  extraOverrides: ""
  extraOverridesConfigmap: ""
  extraOverridesSecret: ""
  extraVolumeMounts: []
  extraVolumes: []
  fullnameOverride: ""
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
  hostAliases: []
  image:
    debug: false
    digest: ""
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/clickhouse
    tag: 22.9.4-debian-11-r1
  ingress:
    annotations: {}
    apiVersion: ""
    enabled: false
    extraHosts: []
    extraPaths: []
    extraRules: []
    extraTls: []
    hostname: clickhouse.local
    ingressClassName: ""
    path: /
    pathType: ImplementationSpecific
    secrets: []
    selfSigned: false
    tls: false
  initContainers: []
  initdbScripts: {}
  initdbScriptsSecret: ""
  kubeVersion: ""
  lifecycleHooks: {}
  livenessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  logLevel: information
  metrics:
    enabled: false
    podAnnotations:
      prometheus.io/port: '{{ .Values.containerPorts.metrics }}'
      prometheus.io/scrape: "true"
    serviceMonitor:
      annotations: {}
      enabled: false
      honorLabels: false
      interval: ""
      jobLabel: ""
      labels: {}
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scrapeTimeout: ""
      selector: {}
  nameOverride: ""
  namespaceOverride: ""
  nodeAffinityPreset:
    key: ""
    type: ""
    values: []
  nodeSelector: {}
  persistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    dataSource: {}
    enabled: true
    selector: {}
    size: 8Gi
    storageClass: ""
  podAffinityPreset: ""
  podAnnotations: {}
  podAntiAffinityPreset: soft
  podLabels: {}
  podManagementPolicy: Parallel
  podSecurityContext:
    enabled: true
    fsGroup: 1001
    seccompProfile:
      type: RuntimeDefault
  priorityClassName: ""
  readinessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  replicaCount: 2
  resources:
    limits: {}
    requests: {}
  schedulerName: ""
  service:
    annotations: {}
    clusterIP: ""
    externalTrafficPolicy: Cluster
    extraPorts: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    nodePorts:
      http: ""
      interserver: ""
      metrics: ""
      mysql: ""
      postgresql: ""
      tcp: ""
      tcpSecure: ""
    ports:
      http: 8123
      interserver: 9009
      metrics: 8001
      mysql: 9004
      postgresql: 9005
      tcp: 9000
      tcpSecure: 9440
    sessionAffinity: None
    sessionAffinityConfig: {}
    type: ClusterIP
  serviceAccount:
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: ""
  shards: 2
  sidecars: []
  startdbScripts: {}
  startdbScriptsSecret: ""
  startupProbe:
    enabled: false
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  terminationGracePeriodSeconds: ""
  tls:
    autoGenerated: false
    certCAFilename: ""
    certFilename: ""
    certKeyFilename: ""
    certificatesSecret: ""
    enabled: false
  tolerations: []
  topologySpreadConstraints: []
  updateStrategy:
    type: RollingUpdate
  volumePermissions:
    containerSecurityContext:
      runAsUser: 0
    enabled: false
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 11-debian-11-r45
    resources:
      limits: {}
      requests: {}
  zookeeper:
    affinity: {}
    args: []
    auth:
      client:
        clientPassword: ""
        clientUser: ""
        enabled: false
        existingSecret: ""
        serverPasswords: ""
        serverUsers: ""
      quorum:
        enabled: false
        existingSecret: ""
        learnerPassword: ""
        learnerUser: ""
        serverPasswords: ""
        serverUsers: ""
    autopurge:
      purgeInterval: 0
      snapRetainCount: 3
    clusterDomain: cluster.local
    command:
    - /scripts/setup.sh
    common:
      exampleValue: common-chart
      global:
        imagePullSecrets: []
        imageRegistry: ""
        storageClass: ""
    commonAnnotations: {}
    commonLabels: {}
    configuration: ""
    containerPorts:
      client: 2181
      election: 3888
      follower: 2888
      tls: 3181
    containerSecurityContext:
      allowPrivilegeEscalation: false
      enabled: true
      runAsNonRoot: true
      runAsUser: 1001
    customLivenessProbe: {}
    customReadinessProbe: {}
    customStartupProbe: {}
    dataLogDir: ""
    diagnosticMode:
      args:
      - infinity
      command:
      - sleep
      enabled: false
    enabled: true
    existingConfigmap: ""
    extraDeploy: []
    extraEnvVars: []
    extraEnvVarsCM: ""
    extraEnvVarsSecret: ""
    extraVolumeMounts: []
    extraVolumes: []
    fourlwCommandsWhitelist: srvr, mntr, ruok
    fullnameOverride: ""
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
    heapSize: 1024
    hostAliases: []
    image:
      debug: false
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/zookeeper
      tag: 3.8.0-debian-11-r47
    initContainers: []
    initLimit: 10
    jvmFlags: ""
    kubeVersion: ""
    lifecycleHooks: {}
    listenOnAllIPs: false
    livenessProbe:
      enabled: true
      failureThreshold: 6
      initialDelaySeconds: 30
      periodSeconds: 10
      probeCommandTimeout: 2
      successThreshold: 1
      timeoutSeconds: 5
    logLevel: ERROR
    maxClientCnxns: 60
    maxSessionTimeout: 40000
    metrics:
      containerPort: 9141
      enabled: false
      prometheusRule:
        additionalLabels: {}
        enabled: false
        namespace: ""
        rules: []
      service:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: '{{ .Values.metrics.service.port }}'
          prometheus.io/scrape: "true"
        port: 9141
        type: ClusterIP
      serviceMonitor:
        additionalLabels: {}
        enabled: false
        honorLabels: false
        interval: ""
        jobLabel: ""
        metricRelabelings: []
        namespace: ""
        relabelings: []
        scrapeTimeout: ""
        selector: {}
    minServerId: 1
    nameOverride: ""
    namespaceOverride: ""
    networkPolicy:
      allowExternal: true
      enabled: false
    nodeAffinityPreset:
      key: ""
      type: ""
      values: []
    nodeSelector: {}
    pdb:
      create: false
      maxUnavailable: 1
      minAvailable: ""
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      dataLogDir:
        existingClaim: ""
        selector: {}
        size: 8Gi
      enabled: true
      existingClaim: ""
      selector: {}
      size: 8Gi
      storageClass: ""
    podAffinityPreset: ""
    podAnnotations: {}
    podAntiAffinityPreset: soft
    podLabels: {}
    podManagementPolicy: Parallel
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    preAllocSize: 65536
    priorityClassName: ""
    readinessProbe:
      enabled: true
      failureThreshold: 6
      initialDelaySeconds: 5
      periodSeconds: 10
      probeCommandTimeout: 2
      successThreshold: 1
      timeoutSeconds: 5
    replicaCount: 2
    resources:
      limits: {}
      requests:
        cpu: 250m
        memory: 256Mi
    schedulerName: ""
    service:
      annotations: {}
      clusterIP: ""
      disableBaseClientPort: false
      externalTrafficPolicy: Cluster
      extraPorts: []
      headless:
        annotations: {}
        publishNotReadyAddresses: true
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        client: ""
        tls: ""
      ports:
        client: 2181
        election: 3888
        follower: 2888
        tls: 3181
      sessionAffinity: None
      sessionAffinityConfig: {}
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: true
      create: false
      name: ""
    sidecars: []
    snapCount: 100000
    startupProbe:
      enabled: false
      failureThreshold: 15
      initialDelaySeconds: 30
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    syncLimit: 5
    tickTime: 2000
    tls:
      client:
        auth: none
        autoGenerated: false
        enabled: false
        existingSecret: ""
        existingSecretKeystoreKey: ""
        existingSecretTruststoreKey: ""
        keystorePassword: ""
        keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
        passwordsSecretKeystoreKey: ""
        passwordsSecretName: ""
        passwordsSecretTruststoreKey: ""
        truststorePassword: ""
        truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
      quorum:
        auth: none
        autoGenerated: false
        enabled: false
        existingSecret: ""
        existingSecretKeystoreKey: ""
        existingSecretTruststoreKey: ""
        keystorePassword: ""
        keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
        passwordsSecretKeystoreKey: ""
        passwordsSecretName: ""
        passwordsSecretTruststoreKey: ""
        truststorePassword: ""
        truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
      resources:
        limits: {}
        requests: {}
    tolerations: []
    topologySpreadConstraints: []
    updateStrategy:
      rollingUpdate: {}
      type: RollingUpdate
    volumePermissions:
      containerSecurityContext:
        enabled: true
        runAsUser: 0
      enabled: false
      image:
        digest: ""
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/bitnami-shell
        tag: 11-debian-11-r42
      resources:
        limits: {}
        requests: {}
opentelemetry-collector:
  affinity: {}
  annotations: {}
  autoscaling:
    enabled: false
    maxReplicas: 10
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
  clusterRole:
    annotations: {}
    clusterRoleBinding:
      annotations: {}
      name: ""
    create: false
    name: ""
    rules: []
  command:
    extraArgs: []
    name: otelcol-contrib
  config:
    exporters:
      clickhouse:
        dsn: tcp://{{ .Release.Name }}-clickhouse-headless:9000/otel
        logs_table_name: otel_logs
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_elapsed_time: 300s
          max_interval: 30s
        sending_queue:
          queue_size: 100
        timeout: 10s
        traces_table_name: otel_traces
        ttl_days: 3
      logging: {}
    extensions:
      health_check: {}
      memory_ballast: {}
      query:
        logging_query:
          storage_type: clickhouse
        metrics_query:
          storage_type: clickhouse
        protocols:
          grpc:
            endpoint: 0.0.0.0:18889
          http:
            endpoint: 0.0.0.0:18888
        storage:
          clickhouse:
            dsn: tcp://{{ .Release.Name }}-clickhouse-headless:9000/otel
            timeout: 5s
            ttl_days: 3
        tracing_query:
          storage_type: clickhouse
    processors:
      batch: {}
      memory_limiter: null
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_http:
            endpoint: 0.0.0.0:14268
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${MY_POD_IP}:8888
      zipkin:
        endpoint: 0.0.0.0:9411
    service:
      extensions:
      - health_check
      - memory_ballast
      pipelines:
        logs:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - prometheus
        traces:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: 0.0.0.0:8888
  configMap:
    create: true
  dnsPolicy: ""
  enabled: true
  extraEnvs: []
  extraVolumeMounts: []
  extraVolumes: []
  fullnameOverride: ""
  global: {}
  hostNetwork: false
  image:
    pullPolicy: IfNotPresent
    repository: ghcr.io/openinsight-proj/openinsight
    tag: v0.62.1
  imagePullSecrets: []
  ingress:
    additionalIngresses: []
    enabled: false
  initContainers: []
  lifecycleHooks: {}
  mode: deployment
  nameOverride: ""
  nodeSelector: {}
  podAnnotations: {}
  podDisruptionBudget:
    enabled: false
  podLabels: {}
  podMonitor:
    enabled: false
    extraLabels: {}
    metricsEndpoints:
    - port: metrics
  podSecurityContext: {}
  ports:
    jaeger-compact:
      containerPort: 6831
      enabled: true
      hostPort: 6831
      protocol: UDP
      servicePort: 6831
    jaeger-grpc:
      containerPort: 14250
      enabled: true
      hostPort: 14250
      protocol: TCP
      servicePort: 14250
    jaeger-thrift:
      containerPort: 14268
      enabled: true
      hostPort: 14268
      protocol: TCP
      servicePort: 14268
    metrics:
      containerPort: 8888
      enabled: false
      protocol: TCP
      servicePort: 8888
    otlp:
      containerPort: 4317
      enabled: true
      hostPort: 4317
      protocol: TCP
      servicePort: 4317
    otlp-http:
      containerPort: 4318
      enabled: true
      hostPort: 4318
      protocol: TCP
      servicePort: 4318
    zipkin:
      containerPort: 9411
      enabled: true
      hostPort: 9411
      protocol: TCP
      servicePort: 9411
  presets:
    clusterMetrics:
      enabled: false
    hostMetrics:
      enabled: false
    kubeletMetrics:
      enabled: false
    kubernetesAttributes:
      enabled: false
    logsCollection:
      enabled: false
      includeCollectorLogs: false
  priorityClassName: ""
  prometheusRule:
    defaultRules:
      enabled: false
    enabled: false
    extraLabels: {}
    groups: []
  replicaCount: 1
  resources:
    limits:
      cpu: 256m
      memory: 512Mi
  rollout:
    rollingUpdate: {}
    strategy: RollingUpdate
  securityContext: {}
  service:
    annotations: {}
    type: ClusterIP
  serviceAccount:
    annotations: {}
    create: true
    name: ""
  serviceMonitor:
    enabled: false
    extraLabels: {}
    metricsEndpoints:
    - port: metrics
  statefulset:
    podManagementPolicy: Parallel
    volumeClaimTemplates: []
  tolerations: []

HOOKS:
MANIFEST:
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myotel-clickhouse
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
automountServiceAccountToken: true
---
# Source: openinsight-helm-chart/charts/opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myotel-opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.38.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/version: "0.63.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myotel-clickhouse
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
type: Opaque
data:
  admin-password: "aVpoczlCRDd4dg=="
---
# Source: openinsight-helm-chart/charts/clickhouse/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myotel-zookeeper-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.4
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myotel-clickhouse
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
data:
  00_default_overrides.xml: |
    <clickhouse>
      <!-- Macros -->
      <macros>
        <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
        <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
        <layer>myotel-clickhouse</layer>
      </macros>
      <!-- Log Level -->
      <logger>
        <level>information</level>
      </logger>
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <default>
          <shard>
              <replica>
                  <host>myotel-clickhouse-shard0-0.myotel-clickhouse-headless.default.svc.cluster.local</host>
                  <port>9000</port>
              </replica>
              <replica>
                  <host>myotel-clickhouse-shard0-1.myotel-clickhouse-headless.default.svc.cluster.local</host>
                  <port>9000</port>
              </replica>
          </shard>
          <shard>
              <replica>
                  <host>myotel-clickhouse-shard1-0.myotel-clickhouse-headless.default.svc.cluster.local</host>
                  <port>9000</port>
              </replica>
              <replica>
                  <host>myotel-clickhouse-shard1-1.myotel-clickhouse-headless.default.svc.cluster.local</host>
                  <port>9000</port>
              </replica>
          </shard>
        </default>
      </remote_servers>
      <!-- Zookeeper configuration -->
      <zookeeper>
        
        <node>
          <host>myotel-zookeeper-0.myotel-zookeeper-headless.default.svc.cluster.local</host>
          <port>2181</port>
        </node>
        <node>
          <host>myotel-zookeeper-1.myotel-zookeeper-headless.default.svc.cluster.local</host>
          <port>2181</port>
        </node>
      </zookeeper>
    </clickhouse>
---
# Source: openinsight-helm-chart/charts/opentelemetry-collector/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myotel-opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.38.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/version: "0.63.1"
    app.kubernetes.io/managed-by: Helm
data:
  relay: |
    exporters:
      clickhouse:
        dsn: tcp://myotel-clickhouse-headless:9000/otel
        logs_table_name: otel_logs
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_elapsed_time: 300s
          max_interval: 30s
        sending_queue:
          queue_size: 100
        timeout: 10s
        traces_table_name: otel_traces
        ttl_days: 3
      logging: {}
    extensions:
      health_check: {}
      memory_ballast: {}
      query:
        logging_query:
          storage_type: clickhouse
        metrics_query:
          storage_type: clickhouse
        protocols:
          grpc:
            endpoint: 0.0.0.0:18889
          http:
            endpoint: 0.0.0.0:18888
        storage:
          clickhouse:
            dsn: tcp://myotel-clickhouse-headless:9000/otel
            timeout: 5s
            ttl_days: 3
        tracing_query:
          storage_type: clickhouse
    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_mib: 409
        spike_limit_mib: 128
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_http:
            endpoint: 0.0.0.0:14268
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${MY_POD_IP}:8888
      zipkin:
        endpoint: 0.0.0.0:9411
    service:
      extensions:
      - health_check
      - memory_ballast
      pipelines:
        logs:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - prometheus
        traces:
          exporters:
          - logging
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: 0.0.0.0:8888
---
# Source: openinsight-helm-chart/charts/clickhouse/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: myotel-zookeeper-headless
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.4
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/component: zookeeper
---
# Source: openinsight-helm-chart/charts/clickhouse/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myotel-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.4
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/component: zookeeper
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: myotel-clickhouse-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: http
      targetPort: http
      port: 8123
      protocol: TCP
    - name: tcp
      targetPort: tcp
      port: 9000
      protocol: TCP
    - name: mysql
      targetPort: mysql
      port: 9004
      protocol: TCP
    - name: tcp-postgresql
      targetPort: tcp-postgresql
      port: 9005
      protocol: TCP
    - name: http-intersrv
      targetPort: http-intersrv
      port: 9009
      protocol: TCP
  selector:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/component: clickhouse
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myotel-clickhouse
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      targetPort: http
      port: 8123
      protocol: TCP
      nodePort: null
    - name: tcp
      targetPort: tcp
      port: 9000
      protocol: TCP
      nodePort: null
    - name: tcp-mysql
      targetPort: tcp-mysql
      port: 9004
      protocol: TCP
      nodePort: null
    - name: tcp-postgresql
      targetPort: tcp-postgresql
      port: 9005
      protocol: TCP
      nodePort: null
    - name: http-intersrv
      targetPort: http-intersrv
      port: 9009
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/component: clickhouse
---
# Source: openinsight-helm-chart/charts/opentelemetry-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myotel-opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.38.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/version: "0.63.1"
    app.kubernetes.io/managed-by: Helm
    component: standalone-collector
spec:
  type: ClusterIP
  ports: 
    
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-thrift
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  selector:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: myotel
    component: standalone-collector
---
# Source: openinsight-helm-chart/charts/opentelemetry-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myotel-opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.38.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/version: "0.63.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: myotel
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 9d808aa2b471976716a338f78ed2e6a032d3e65eadebf099ff6d63d6649d69a4
        
      labels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/instance: myotel
        component: standalone-collector
        
    spec:
      
      serviceAccountName: myotel-opentelemetry-collector
      securityContext:
        {}
      containers:
        - name: opentelemetry-collector
          command:
            - /otelcol-contrib
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "ghcr.io/openinsight-proj/openinsight:v0.62.1"
          imagePullPolicy: IfNotPresent
          ports:
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: zipkin
              containerPort: 9411
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              cpu: 256m
              memory: 512Mi
          volumeMounts:
            - mountPath: /conf
              name: opentelemetry-collector-configmap
      volumes:
        - name: opentelemetry-collector-configmap
          configMap:
            name: myotel-opentelemetry-collector
            items:
              - key: relay
                path: relay.yaml
---
# Source: openinsight-helm-chart/charts/clickhouse/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myotel-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.4
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 2
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: myotel
      app.kubernetes.io/component: zookeeper
  serviceName: myotel-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-10.2.4
        app.kubernetes.io/instance: myotel
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: myotel
                    app.kubernetes.io/component: zookeeper
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-11-r47
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: myotel-zookeeper-0.myotel-zookeeper-headless.default.svc.cluster.local:2888:3888::1 myotel-zookeeper-1.myotel-zookeeper-headless.default.svc.cluster.local:2888:3888::2 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: myotel-zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myotel-clickhouse-shard0
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
spec:
  replicas: 2
  podManagementPolicy: "Parallel"
  selector:
    matchLabels: 
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/instance: myotel
      app.kubernetes.io/component: clickhouse
  serviceName: myotel-clickhouse-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clickhouse
        helm.sh/chart: clickhouse-1.0.3
        app.kubernetes.io/instance: myotel
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: clickhouse
    spec:
      serviceAccountName: myotel-clickhouse
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: clickhouse
                    app.kubernetes.io/instance: myotel
                    app.kubernetes.io/component: clickhouse
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
      containers:
        - name: clickhouse
          image: docker.io/bitnami/clickhouse:22.9.4-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_TCP_PORT
              value: "9000"
            - name: CLICKHOUSE_MYSQL_PORT
              value: "9004"
            - name: CLICKHOUSE_POSTGRESQL_PORT
              value: "9005"
            - name: CLICKHOUSE_INTERSERVER_HTTP_PORT
              value: "9009"
            - name: CLICKHOUSE_ADMIN_USER
              value: "default"
            - name: CLICKHOUSE_SHARD_ID
              value: "shard0"
            - name: CLICKHOUSE_REPLICA_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CLICKHOUSE_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myotel-clickhouse
                  key: admin-password
          envFrom:
          resources:
            limits: {}
            requests: {}
          ports:
            - name: http
              containerPort: 8123
            - name: tcp
              containerPort: 9000
            - name: tcp-postgresql
              containerPort: 9005
            - name: tcp-mysql
              containerPort: 9004
            - name: http-intersrv
              containerPort: 9009
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /ping
              port: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /ping
              port: http
          volumeMounts:
            - name: data
              mountPath: /bitnami/clickhouse
            - name: config
              mountPath: /bitnami/clickhouse/etc/conf.d/default
      volumes:
        - name: config
          configMap:
            name: myotel-clickhouse
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: openinsight-helm-chart/charts/clickhouse/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myotel-clickhouse-shard1
  namespace: "default"
  labels:
    app.kubernetes.io/name: clickhouse
    helm.sh/chart: clickhouse-1.0.3
    app.kubernetes.io/instance: myotel
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: clickhouse
spec:
  replicas: 2
  podManagementPolicy: "Parallel"
  selector:
    matchLabels: 
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/instance: myotel
      app.kubernetes.io/component: clickhouse
  serviceName: myotel-clickhouse-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clickhouse
        helm.sh/chart: clickhouse-1.0.3
        app.kubernetes.io/instance: myotel
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: clickhouse
    spec:
      serviceAccountName: myotel-clickhouse
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: clickhouse
                    app.kubernetes.io/instance: myotel
                    app.kubernetes.io/component: clickhouse
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
      containers:
        - name: clickhouse
          image: docker.io/bitnami/clickhouse:22.9.4-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_TCP_PORT
              value: "9000"
            - name: CLICKHOUSE_MYSQL_PORT
              value: "9004"
            - name: CLICKHOUSE_POSTGRESQL_PORT
              value: "9005"
            - name: CLICKHOUSE_INTERSERVER_HTTP_PORT
              value: "9009"
            - name: CLICKHOUSE_ADMIN_USER
              value: "default"
            - name: CLICKHOUSE_SHARD_ID
              value: "shard1"
            - name: CLICKHOUSE_REPLICA_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CLICKHOUSE_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myotel-clickhouse
                  key: admin-password
          envFrom:
          resources:
            limits: {}
            requests: {}
          ports:
            - name: http
              containerPort: 8123
            - name: tcp
              containerPort: 9000
            - name: tcp-postgresql
              containerPort: 9005
            - name: tcp-mysql
              containerPort: 9004
            - name: http-intersrv
              containerPort: 9009
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /ping
              port: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /ping
              port: http
          volumeMounts:
            - name: data
              mountPath: /bitnami/clickhouse
            - name: config
              mountPath: /bitnami/clickhouse/etc/conf.d/default
      volumes:
        - name: config
          configMap:
            name: myotel-clickhouse
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"

